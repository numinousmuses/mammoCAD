{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Note that this code was run in a Kaggle Kernel using TPU as the accelerator. \n#Running the exact code in other environments may not work\n#This code was inspired by this work of Amy Jang here: https://www.kaggle.com/code/amyjang/tensorflow-pneumonia-classification-on-x-rays\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path()\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [512, 512]\nEPOCHS = 100\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames = tf.io.gfile.glob(str(GCS_PATH + '/MINI-DDSM CANCER-NORMAL DATASET/Cancer/*.jpg'))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + '/MINI-DDSM CANCER-NORMAL DATASET/Normal/*.jpg')))\n\ntrain_filenames, val_filenames = train_test_split(filenames, test_size=0.2)\n\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Get the credential from the Cloud SDK\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\n\n# Step 2: Set the credentials\nuser_secrets.set_tensorflow_credential(user_credential)\n\n# Step 3: Use a familiar call to get the GCS path of the dataset\nfrom kaggle_datasets import KaggleDatasets\nGCS_PATH = KaggleDatasets().get_gcs_path()\n    \nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COUNT_NORMAL = len([filename for filename in train_filenames if \"Normal\" in filename])\nprint(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n\nCOUNT_CANCER = len([filename for filename in train_filenames if \"Cancer\" in filename])\nprint(\"Cancer images count in training set: \" + str(COUNT_CANCER))\n\nCOUNT_NORMAL_V = len([filename for filename in val_filenames if \"Normal\" in filename])\nprint(\"Normal images count in testing set: \" + str(COUNT_NORMAL_V))\n\nCOUNT_CANCER_V = len([filename for filename in val_filenames if \"Cancer\" in filename])\nprint(\"Cancer images count in testing set: \" + str(COUNT_CANCER_V))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\nval_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\n\nfor f in train_list_ds.take(5):\n    print(f.numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\nprint(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n\nVAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\nprint(\"Validating images count: \" + str(VAL_IMG_COUNT))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CLASS_NAMES = np.array([str(tf.strings.split(item, os.path.sep)[-1].numpy())[2:-1]\n                        for item in tf.io.gfile.glob(str(GCS_PATH + \"/MINI-DDSM CANCER-NORMAL DATASET/*\"))])\nCLASS_NAMES","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_label(file_path):\n    # convert the path to a list of path components\n    parts = tf.strings.split(file_path, os.path.sep)\n    # The second to last is the class-directory\n    return parts[-2] == \"Cancer\"\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_img(img):\n  # convert the compressed string to a 3D uint8 tensor\n  img = tf.image.decode_jpeg(img, channels=3)\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n  img = tf.image.convert_image_dtype(img, tf.float32)\n  # resize the image to the desired size.\n  return tf.image.resize(img, IMAGE_SIZE)\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_path(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\nval_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\ntest_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image, label in train_ds.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n    # This is a small dataset, only load it once, and keep it in memory.\n    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n    # fit in memory.\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n\n    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n\n    # Repeat forever\n    ds = ds.repeat()\n\n    ds = ds.batch(BATCH_SIZE)\n\n    # `prefetch` lets the dataset fetch batches in the background while the model\n    # is training.\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n    return ds\n\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = prepare_for_training(train_ds)\nval_ds = prepare_for_training(val_ds)\n\nimage_batch, label_batch = next(iter(train_ds))\n\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10,10))\n    for n in range(25):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        if label_batch[n]:\n            plt.title(\"PNEUMONIA\")\n        else:\n            plt.title(\"NORMAL\")\n        plt.axis(\"off\")\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_batch(image_batch.numpy(), label_batch.numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    model = tf.keras.Sequential([\n        \n        tf.keras.Input(shape=(512, 512, 3)),\n        \n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        tf.keras.layers.SeparableConv2D(32, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(32, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n        \n        tf.keras.layers.SeparableConv2D(64, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(64, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D(),\n        \n        tf.keras.layers.SeparableConv2D(128, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(128, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D(),\n        \n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.SeparableConv2D(256, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(256, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.Flatten(),\n    \n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.7),\n        \n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        \n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        \n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    return model\n\nprint('done')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=METRICS\n    )\n\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    steps_per_epoch=TRAIN_IMG_COUNT // BATCH_SIZE,\n    epochs=200,\n    validation_data=val_ds,\n    validation_steps=VAL_IMG_COUNT // BATCH_SIZE,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[['recall', 'precision']].plot()\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['acc', 'val_acc']].plot()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 4, figsize=(20, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['precision', 'recall', 'accuracy', 'loss']):\n    ax[i].plot(history.history[met])\n    ax[i].plot(history.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Thanks for reading my code, if you plan to include it in your work, please properly cite me\n#Thanks again!","metadata":{},"execution_count":null,"outputs":[]}]}